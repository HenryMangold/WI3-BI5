{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:50:58.786448Z",
     "start_time": "2021-11-29T21:50:57.813532Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define functions for pipeline</h1>\n",
    "<h2>Find start timestamp</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:50:58.792384Z",
     "start_time": "2021-11-29T21:50:58.789753Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_start_date(filename=str):\n",
    "    match = re.search(r'\\d{4}\\d{2}\\d{2}_\\d{2}\\d{2}\\d{2}', filename)\n",
    "    date = datetime.datetime.strptime(match.group(), '%Y%m%d_%H%M%S')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>seperate audio and create dataframes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:51:09.225233Z",
     "start_time": "2021-11-29T21:51:09.217659Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataframes(unpickled, start_time):\n",
    "    # create dataframe without audio records\n",
    "    dict_no_audio = {}\n",
    "    for k in unpickled.keys():\n",
    "        if k != 'audio':\n",
    "            dict_no_audio[k] = unpickled[k]\n",
    "\n",
    "    #################### f端r 1h example ########################\n",
    "    #dict_no_audio['timer'] = np.append(dict_no_audio['timer'], 3599906)\n",
    "    ############################################################\n",
    "        \n",
    "    df_sensor = pd.DataFrame(dict_no_audio)\n",
    "    # change timer to datetime and make it index of the dataframe\n",
    "    df_sensor['timer'] = pd.to_timedelta(df_sensor.timer, unit='ms')\n",
    "    df_sensor['ms_delta'] = df_sensor['timer']\n",
    "    \n",
    "    #df_sensor['datetime'] = df_sensor.apply(lambda row: start_time + datetime.timedelta(milliseconds = row.timer), axis = 1)\n",
    "    #df_sensor.drop(columns = ['timer'], inplace = True)\n",
    "    #df_sensor.set_index('datetime', inplace = True)\n",
    "    \n",
    "    # create Data Frame with audio records\n",
    "    df_audio = pd.DataFrame(unpickled['audio'])\n",
    "    df_audio.rename(columns={0: \"audio\"}, inplace = True)\n",
    "    # create date range\n",
    "    df_audio['datetime'] = pd.date_range(start_time, start_time+df_sensor.timer.max(), periods = len(df_audio.index))\n",
    "    \n",
    "    # set date range as index\n",
    "    df_sensor.set_index('timer', inplace = True)\n",
    "    df_audio.set_index('datetime', inplace = True)\n",
    "    \n",
    "\n",
    "    return df_sensor, df_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Normalized accelerometer/magnetometer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:50:58.850256Z",
     "start_time": "2021-11-29T21:50:58.818397Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_normalized_sensors(df_sensor):\n",
    "    # calculate vektor length of sensors\n",
    "    df_sensor['accelerometer'] = (df_sensor['accelerometer_x']**2 + df_sensor['accelerometer_y']**2 + df_sensor['accelerometer_z']**2)**0.5\n",
    "    df_sensor['magnetometer'] = (df_sensor['magnetometer_x']**2 + df_sensor['magnetometer_y']**2 + df_sensor['magnetometer_z']**2)**0.5\n",
    "    return df_sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Split and claculate values for 20 Hz</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:50:58.890518Z",
     "start_time": "2021-11-29T21:50:58.852609Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resampling(df_sensor, df_audio, start_time):\n",
    "    df_agg = pd.DataFrame()\n",
    "    r_sensor = df_sensor.resample('50ms')\n",
    "    r_audio = df_audio.resample('50ms')\n",
    "    for variable in df_sensor.columns:\n",
    "        if \"accelerometer\" in variable:\n",
    "            # werte f端r das resample objekt berechnen\n",
    "            # maximum\n",
    "            df_agg[variable+'_max'] = r_sensor[variable].max()\n",
    "            # 95% quantile\n",
    "            df_agg[variable+'_95q'] = r_sensor[variable].quantile(0.95)\n",
    "            # minimum\n",
    "            df_agg[variable+'_min'] = r_sensor[variable].min()\n",
    "            # 5% quantile\n",
    "            df_agg[variable+'_05q'] = r_sensor[variable].quantile(0.05)\n",
    "\n",
    "        elif \"magnetometer\" in variable:\n",
    "            # werte f端r das resample objekt berechnen\n",
    "            # mean\n",
    "            df_agg[variable+'_mean'] = r_sensor[variable].mean()\n",
    "            # median\n",
    "            df_agg[variable+'_med'] = r_sensor[variable].median()\n",
    "            \n",
    "    df_agg['datetime'] = pd.date_range(start_time, start_time+df_sensor.ms_delta.max(), freq = '50ms')\n",
    "    df_agg.set_index('datetime', inplace = True)\n",
    "\n",
    "    for variable in df_audio.columns:\n",
    "        if \"audio\" in variable:\n",
    "            # werte f端r das resample objekt berechnen\n",
    "            # maximum\n",
    "            df_agg[variable+'_max'] = r_audio[variable].max()\n",
    "\n",
    "            # 95% quantile\n",
    "            df_agg[variable+'_95q'] = r_audio[variable].quantile(0.95)\n",
    "\n",
    "            \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:50:58.933844Z",
     "start_time": "2021-11-29T21:50:58.892297Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_csv(df_agg, inside=bool):\n",
    "    # create datetime index\n",
    "    \n",
    "    # write dataframe to csv\n",
    "    if inside:\n",
    "        if os.path.isfile('../Data/data_inside.csv'):\n",
    "            df_agg.to_csv('../Data/data_inside.csv', mode='a', header=False)\n",
    "        else:\n",
    "            df_agg.to_csv('../Data/data_inside.csv', mode='w', header=True)\n",
    "    else:\n",
    "        if os.path.isfile('../Data/data_outside.csv'):\n",
    "            df_agg.to_csv('../Data/data_outside.csv', mode='a', header=False)\n",
    "        else:\n",
    "            df_agg.to_csv('../Data/data_outside.csv', mode='w', header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find files in directory and start smoothing pipeline</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-29T21:51:14.528Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file,  1.3565006256103516 seconds\n",
      "found starttime,  1.3566462993621826 seconds\n",
      "dataframes created,  18.007048845291138 seconds\n",
      "added normalized sensors,  20.211562395095825 seconds\n",
      "aggregation,  398.66071605682373 seconds\n",
      "executed \" ../Data/data_inside_20211012_000102.pkl \" in 433.4146296977997 seconds\n",
      "read file,  29.62091612815857 seconds\n",
      "found starttime,  29.621113061904907 seconds\n",
      "dataframes created,  46.07564616203308 seconds\n",
      "added normalized sensors,  48.27023911476135 seconds\n",
      "aggregation,  392.53272581100464 seconds\n",
      "executed \" ../Data/data_outside_20211013_000100.pkl \" in 427.4350702762604 seconds\n",
      "read file,  29.480689525604248 seconds\n",
      "found starttime,  29.480875968933105 seconds\n",
      "dataframes created,  46.20866131782532 seconds\n",
      "added normalized sensors,  48.423357248306274 seconds\n",
      "aggregation,  408.7267711162567 seconds\n",
      "executed \" ../Data/data_inside_20211014_000058.pkl \" in 444.26641273498535 seconds\n",
      "read file,  35.698676109313965 seconds\n",
      "found starttime,  35.69886755943298 seconds\n",
      "dataframes created,  52.53439474105835 seconds\n",
      "added normalized sensors,  54.76284050941467 seconds\n",
      "aggregation,  430.37369871139526 seconds\n",
      "executed \" ../Data/data_inside_20211027_000101.pkl \" in 465.21618032455444 seconds\n",
      "read file,  32.70492196083069 seconds\n",
      "found starttime,  32.705639123916626 seconds\n",
      "dataframes created,  49.44295024871826 seconds\n",
      "added normalized sensors,  51.66151261329651 seconds\n",
      "aggregation,  464.7044951915741 seconds\n",
      "executed \" ../Data/data_inside_20211028_000102.pkl \" in 499.6773386001587 seconds\n",
      "read file,  10.549779415130615 seconds\n",
      "found starttime,  10.550108432769775 seconds\n",
      "dataframes created,  16.205966472625732 seconds\n",
      "added normalized sensors,  16.951528549194336 seconds\n",
      "aggregation,  125.72568559646606 seconds\n",
      "executed \" ../Data/data_inside_20211024_160007.pkl \" in 137.28438878059387 seconds\n",
      "read file,  30.534080266952515 seconds\n",
      "found starttime,  30.534276485443115 seconds\n",
      "dataframes created,  46.99024701118469 seconds\n",
      "added normalized sensors,  49.15833115577698 seconds\n",
      "aggregation,  411.8378312587738 seconds\n",
      "executed \" ../Data/data_outside_20211012_000102.pkl \" in 447.41812896728516 seconds\n",
      "read file,  30.30328369140625 seconds\n",
      "found starttime,  30.303478240966797 seconds\n",
      "dataframes created,  46.91944932937622 seconds\n",
      "added normalized sensors,  49.14134955406189 seconds\n",
      "aggregation,  420.5095250606537 seconds\n",
      "executed \" ../Data/data_inside_20211021_000103.pkl \" in 455.3208510875702 seconds\n",
      "read file,  30.017646551132202 seconds\n",
      "found starttime,  30.017824172973633 seconds\n",
      "dataframes created,  46.49488282203674 seconds\n",
      "added normalized sensors,  48.67031693458557 seconds\n",
      "aggregation,  419.8810226917267 seconds\n",
      "executed \" ../Data/data_inside_20211015_000106.pkl \" in 454.63151502609253 seconds\n",
      "read file,  31.343628644943237 seconds\n",
      "found starttime,  31.343815803527832 seconds\n",
      "dataframes created,  47.13859009742737 seconds\n",
      "added normalized sensors,  49.21266198158264 seconds\n",
      "aggregation,  395.69188594818115 seconds\n",
      "executed \" ../Data/data_outside_20211027_000101.pkl \" in 431.2535753250122 seconds\n",
      "read file,  28.02177143096924 seconds\n",
      "found starttime,  28.021955251693726 seconds\n",
      "dataframes created,  43.572059631347656 seconds\n",
      "added normalized sensors,  45.62604331970215 seconds\n",
      "aggregation,  385.3404471874237 seconds\n",
      "executed \" ../Data/data_outside_20211025_000059.pkl \" in 420.84057235717773 seconds\n",
      "read file,  29.54256844520569 seconds\n",
      "found starttime,  29.54275155067444 seconds\n",
      "dataframes created,  46.30456614494324 seconds\n",
      "added normalized sensors,  48.520224809646606 seconds\n",
      "aggregation,  412.80916929244995 seconds\n",
      "executed \" ../Data/data_inside_20211025_000059.pkl \" in 447.8217842578888 seconds\n",
      "read file,  29.009271144866943 seconds\n",
      "found starttime,  29.009450912475586 seconds\n",
      "dataframes created,  45.389681339263916 seconds\n",
      "added normalized sensors,  47.551060914993286 seconds\n",
      "aggregation,  433.3447823524475 seconds\n",
      "executed \" ../Data/data_inside_20211020_000100.pkl \" in 468.1006021499634 seconds\n",
      "read file,  22.911256551742554 seconds\n",
      "found starttime,  22.911481618881226 seconds\n",
      "dataframes created,  33.990092515945435 seconds\n",
      "added normalized sensors,  35.45659399032593 seconds\n",
      "aggregation,  270.45322608947754 seconds\n",
      "executed \" ../Data/data_outside_20211016_000100.pkl \" in 294.1221227645874 seconds\n",
      "read file,  21.97300410270691 seconds\n",
      "found starttime,  21.973219394683838 seconds\n",
      "dataframes created,  33.14751839637756 seconds\n",
      "added normalized sensors,  34.627859354019165 seconds\n",
      "aggregation,  271.792325258255 seconds\n",
      "executed \" ../Data/data_inside_20211023_000102.pkl \" in 295.0644519329071 seconds\n",
      "read file,  21.528273820877075 seconds\n",
      "found starttime,  21.528692483901978 seconds\n",
      "dataframes created,  32.566712856292725 seconds\n",
      "added normalized sensors,  34.03234887123108 seconds\n",
      "aggregation,  269.91060090065 seconds\n",
      "executed \" ../Data/data_inside_20211016_000100.pkl \" in 293.0153868198395 seconds\n",
      "read file,  9.516890525817871 seconds\n",
      "found starttime,  9.517087936401367 seconds\n",
      "dataframes created,  14.886716365814209 seconds\n",
      "added normalized sensors,  15.594563484191895 seconds\n",
      "aggregation,  119.01612520217896 seconds\n",
      "executed \" ../Data/data_outside_20211024_160007.pkl \" in 130.80712389945984 seconds\n",
      "read file,  30.333815336227417 seconds\n",
      "found starttime,  30.334026336669922 seconds\n",
      "dataframes created,  46.576645612716675 seconds\n",
      "added normalized sensors,  48.7250611782074 seconds\n",
      "aggregation,  862.4375507831573 seconds\n",
      "executed \" ../Data/data_outside_20211026_000102.pkl \" in 898.136212348938 seconds\n",
      "read file,  29.85337734222412 seconds\n",
      "found starttime,  29.853896141052246 seconds\n",
      "dataframes created,  46.473214864730835 seconds\n",
      "added normalized sensors,  48.66285228729248 seconds\n",
      "aggregation,  510.5798568725586 seconds\n",
      "executed \" ../Data/data_outside_20211020_000100.pkl \" in 546.2082369327545 seconds\n",
      "read file,  29.590677738189697 seconds\n",
      "found starttime,  29.590847492218018 seconds\n",
      "dataframes created,  46.340434074401855 seconds\n",
      "added normalized sensors,  48.55349087715149 seconds\n",
      "aggregation,  422.96253490448 seconds\n",
      "executed \" ../Data/data_inside_20211022_000103.pkl \" in 457.8065359592438 seconds\n",
      "read file,  28.782130479812622 seconds\n",
      "found starttime,  28.78230929374695 seconds\n",
      "dataframes created,  44.258355379104614 seconds\n",
      "added normalized sensors,  46.296842098236084 seconds\n",
      "aggregation,  382.5692102909088 seconds\n",
      "executed \" ../Data/data_outside_20211014_000058.pkl \" in 418.2471227645874 seconds\n",
      "read file,  29.312166690826416 seconds\n",
      "found starttime,  29.31260871887207 seconds\n",
      "dataframes created,  45.91086387634277 seconds\n",
      "added normalized sensors,  48.11521887779236 seconds\n"
     ]
    }
   ],
   "source": [
    "data_inside = []\n",
    "data_outside = []\n",
    "\n",
    "for path, currentDirectory, files in os.walk(\"../Data/\"):\n",
    "    for file in files:\n",
    "        if file.startswith(\"data_inside_\") and not file.endswith('_one_hour_example.pkl'):\n",
    "            time_s = time.time()\n",
    "            \n",
    "            unpickled = pd.read_pickle(path+file)\n",
    "            print('read file, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # find starting time of recording\n",
    "            start_time = find_start_date(file)\n",
    "            print('found starttime, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # create data frames for sensors and audio with timer as index\n",
    "            df_sensor, df_audio = create_dataframes(unpickled, start_time)\n",
    "            print('dataframes created, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # add vektor length for sensors as variable\n",
    "            df_sensor = add_normalized_sensors(df_sensor)\n",
    "            print('added normalized sensors, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # resample to 20 Hz\n",
    "            df_agg = resampling(df_sensor, df_audio, start_time)\n",
    "            print('aggregation, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # write to csv file\n",
    "            save_to_csv(df_agg, inside=True)\n",
    "            \n",
    "            print('executed \"', path+file, '\" in %s seconds' % (time.time() - time_s))\n",
    "            \n",
    "        elif file.startswith(\"data_outside_\") and not file.endswith('_one_hour_example.pkl'):\n",
    "            time_s = time.time()\n",
    "            \n",
    "            unpickled = pd.read_pickle(path+file)\n",
    "            print('read file, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # find starting time of recording\n",
    "            start_time = find_start_date(file)\n",
    "            print('found starttime, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # create data frames for sensors and audio with timer as index\n",
    "            df_sensor, df_audio = create_dataframes(unpickled, start_time)\n",
    "            print('dataframes created, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # add vektor length for sensors as variable\n",
    "            df_sensor = add_normalized_sensors(df_sensor)\n",
    "            print('added normalized sensors, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # resample to 20 Hz\n",
    "            df_agg = resampling(df_sensor, df_audio, start_time)\n",
    "            print('aggregation, ', '%s seconds' % (time.time() - time_s))\n",
    "            \n",
    "            # write to csv file\n",
    "            save_to_csv(df_agg, inside=False)\n",
    "            \n",
    "            print('executed \"', path+file, '\" in %s seconds' % (time.time() - time_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:51:00.666033Z",
     "start_time": "2021-11-29T21:51:00.666019Z"
    }
   },
   "outputs": [],
   "source": [
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:51:00.666846Z",
     "start_time": "2021-11-29T21:51:00.666833Z"
    }
   },
   "outputs": [],
   "source": [
    "df_audio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
