{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T08:27:28.303827Z",
     "start_time": "2021-11-29T08:27:27.328170Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define functions for pipeline</h1>\n",
    "<h2>Find start timestamp</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T09:46:32.520259Z",
     "start_time": "2021-11-29T09:46:32.515258Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_start_date(filename=str):\n",
    "    match = re.search(r'\\d{4}\\d{2}\\d{2}_\\d{2}\\d{2}\\d{2}', filename)\n",
    "    date = datetime.datetime.strptime(match.group(), '%Y%m%d_%H%M%S')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define cutting points</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T09:46:38.045923Z",
     "start_time": "2021-11-29T09:46:38.034922Z"
    }
   },
   "outputs": [],
   "source": [
    "def cutting_points(unpickled):\n",
    "    # create dataframe without audio records for split-steps von\n",
    "    dict_no_audio = {}\n",
    "    for k in unpickled.keys():\n",
    "        if k != 'audio':\n",
    "            dict_no_audio[k] = unpickled[k]\n",
    "\n",
    "    #################### f√ºr 1h example ########################\n",
    "    #dict_no_audio['timer'] = np.append(dict_no_audio['timer'], 3599906)\n",
    "    ############################################################\n",
    "    \n",
    "    \n",
    "    df_no_audio = pd.DataFrame()\n",
    "    # define range for split-steps of sensor records\n",
    "    #val_range = np.array(range(49,unpickled['timer'].max(),50))\n",
    "    \n",
    "    # split-steps of sensor records\n",
    "    #i_split_arr = []\n",
    "\n",
    "    #for ms in val_range:\n",
    "    #    i_split_arr.append(dict_no_audio['timer'].searchsorted(ms, side='right'))\n",
    "    \n",
    "    # split-step of audio records\n",
    "    step = int(2100 / 20)\n",
    "\n",
    "    return step, df_no_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Normalized accelerometer/magnetometer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T08:27:40.189593Z",
     "start_time": "2021-11-29T08:27:40.171879Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_normalized_sensors(unpickled):\n",
    "    # calculate vektor length of sensors\n",
    "    unpickled['accelerometer'] = (unpickled['accelerometer_x']**2 + unpickled['accelerometer_y']**2 + unpickled['accelerometer_z']**2)**0.5\n",
    "    unpickled['magnetometer'] = (unpickled['magnetometer_x']**2 + unpickled['magnetometer_y']**2 + unpickled['magnetometer_z']**2)**0.5\n",
    "    return unpickled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Split and claculate values for 20 Hz</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T10:24:52.681021Z",
     "start_time": "2021-11-29T10:24:52.581349Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def smoothing(unpickled, i_split_arr, step):\n",
    "    # create dictionary with records (run split) & calculate min/max/quantil\n",
    "\n",
    "    smt_dict = {}\n",
    "\n",
    "    for variable in unpickled.keys():\n",
    "        if \"accelerometer\" in variable:\n",
    "            # Array wird anhand der Indizes der MS gesplittet, mit NaN values auf die selbe lenge gebracht und gestacked\n",
    "            ac_split_arr = np.split(unpickled[variable], i_split_arr)\n",
    "            pad = len(max(ac_split_arr, key=len))\n",
    "            pad_ac_split_arr = np.array([np.append(i, [[np.nan]*(pad-len(i))]) for i in ac_split_arr])\n",
    "            stack_ac_split_arr = np.vstack(pad_ac_split_arr)\n",
    "\n",
    "            # Zusammenfassen und in dictionary schreiben, NaN values werden ignoriert\n",
    "            # maximum\n",
    "            smt_dict[variable+'_max'] = np.nanmax(stack_ac_split_arr, axis = 1)\n",
    "            # 95% quantile\n",
    "            smt_dict[variable+'_95q'] = np.nanquantile(stack_ac_split_arr, 0.95, axis = 1)\n",
    "            # minimum\n",
    "            smt_dict[variable+'_min'] = np.nanmin(stack_ac_split_arr, axis = 1)\n",
    "            # 5% quantile\n",
    "            smt_dict[variable+'_05q'] = np.nanquantile(stack_ac_split_arr, 0.05, axis = 1)\n",
    "\n",
    "        elif \"magnetometer\" in variable:\n",
    "            # Array wird anhand der Indizes der MS gesplittet, mit NaN values auf die selbe lenge gebracht und gestacked\n",
    "            ma_split_arr = np.split(unpickled[variable], i_split_arr)\n",
    "            pad = len(max(ma_split_arr, key=len))\n",
    "            pad_ma_split_arr = np.array([np.append(i, [[np.nan]*(pad-len(i))]) for i in ma_split_arr])\n",
    "            stack_ma_split_arr = np.vstack(pad_ma_split_arr)\n",
    "\n",
    "            # Zusammenfassen und in dictionary schreiben, NaN values werden ignoriert\n",
    "            # mean\n",
    "            smt_dict[variable+'_mean'] = np.nanmean(stack_ma_split_arr, axis = 1)\n",
    "            # median\n",
    "            smt_dict[variable+'_med'] = np.nanmedian(stack_ma_split_arr, axis = 1)\n",
    "\n",
    "        elif \"audio\" in variable:\n",
    "            au_split_arr = np.vstack(np.split(unpickled[variable], range(step, unpickled['audio'].size, step)))\n",
    "\n",
    "            # Zusammenfassen und in DataFrame schreiben, NaN values werden ignoriert\n",
    "            # maximum\n",
    "            smt_dict[variable+'_max'] = np.amax(au_split_arr, axis = 1)\n",
    "\n",
    "            # 95% quantile\n",
    "            smt_dict[variable+'_95q'] = np.quantile(au_split_arr, 0.95, axis = 1)\n",
    "        \n",
    "        elif \"timer\" in variable:\n",
    "            smt_dict[variable] = np.array(range(0,unpickled['timer'].max(),50))\n",
    "            if unpickled['timer'].max()%50 == 0:\n",
    "                smt_dict[variable] = np.append(smt_dict[variable],(smt_dict[variable].max()+1))\n",
    "            \n",
    "    return smt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T00:11:07.951420Z",
     "start_time": "2021-11-25T00:11:07.939193Z"
    }
   },
   "source": [
    "<b>DataFrame erstellen</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T09:46:58.353937Z",
     "start_time": "2021-11-29T09:46:58.339933Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform equality of sensor & audio records\n",
    "def perform_equality(smt_dict):\n",
    "    diff = (smt_dict['audio_max'].size - smt_dict['accelerometer_x_max'].size)\n",
    "    \n",
    "    # more audio records\n",
    "    if diff > 0: \n",
    "        \n",
    "        # trim records under consideration of difference\n",
    "        smt_dict['audio_max'] = smt_dict['audio_max'][:-diff]\n",
    "        smt_dict['audio_95q'] = smt_dict['audio_95q'][:-diff]\n",
    "        \n",
    "        return smt_dict\n",
    "    \n",
    "    # equal number of audio & sensor records\n",
    "    elif diff == 0:\n",
    "        return smt_dict\n",
    "    \n",
    "    else:\n",
    "        print('equality Error - more sensor data than audio recods')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T09:47:01.603400Z",
     "start_time": "2021-11-29T09:47:01.588396Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_csv(smt_dict, start_time, inside=bool):\n",
    "    # cutt arrays to same length and create Data Frame\n",
    "    smt_dict = perform_equality(smt_dict)   \n",
    "    df = pd.DataFrame.from_dict(smt_dict)\n",
    "    \n",
    "    # add timestamp for records\n",
    "    df['datetime'] = df.apply(lambda row: start_time + datetime.timedelta(milliseconds = row.timer), axis = 1)\n",
    "    df.drop(columns = ['timer'], inplace = True)\n",
    "    df.set_index('datetime', inplace = True)\n",
    "    df\n",
    "    \n",
    "    # write dataframe to csv\n",
    "    if inside:\n",
    "        if os.path.isfile('data/data_inside.csv'):\n",
    "            df.to_csv('data/data_inside.csv', mode='a', header=False)\n",
    "        else:\n",
    "            df.to_csv('data/data_inside.csv', mode='w', header=True)\n",
    "    else:\n",
    "        if os.path.isfile('data/data_outside.csv'):\n",
    "            df.to_csv('data/data_outside.csv', mode='a', header=False)\n",
    "        else:\n",
    "            df.to_csv('data/data_outside.csv', mode='w', header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find files in directory and start smoothing pipeline</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-29T10:25:07.897Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:16: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_max'] = np.nanmax(stack_ac_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1389: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:20: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_min'] = np.nanmin(stack_ac_split_arr, axis = 1)\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:33: RuntimeWarning: Mean of empty slice\n",
      "  smt_dict[variable+'_mean'] = np.nanmean(stack_ma_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1113: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed \" data/data_inside_20211012_000102.pkl \" in 1534.4019498825073 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:16: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_max'] = np.nanmax(stack_ac_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1389: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:20: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_min'] = np.nanmin(stack_ac_split_arr, axis = 1)\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:33: RuntimeWarning: Mean of empty slice\n",
      "  smt_dict[variable+'_mean'] = np.nanmean(stack_ma_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1113: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed \" data/data_inside_20211013_000100.pkl \" in 1566.260300397873 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:16: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_max'] = np.nanmax(stack_ac_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1389: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:20: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_min'] = np.nanmin(stack_ac_split_arr, axis = 1)\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:33: RuntimeWarning: Mean of empty slice\n",
      "  smt_dict[variable+'_mean'] = np.nanmean(stack_ma_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1113: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed \" data/data_inside_20211014_000058.pkl \" in 1518.2803301811218 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:16: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_max'] = np.nanmax(stack_ac_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1389: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:20: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_min'] = np.nanmin(stack_ac_split_arr, axis = 1)\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:33: RuntimeWarning: Mean of empty slice\n",
      "  smt_dict[variable+'_mean'] = np.nanmean(stack_ma_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1113: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed \" data/data_inside_20211015_000106.pkl \" in 1548.3978290557861 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:16: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_max'] = np.nanmax(stack_ac_split_arr, axis = 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1389: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n",
      "C:\\Users\\timst\\AppData\\Local\\Temp/ipykernel_11244/583490427.py:20: RuntimeWarning: All-NaN slice encountered\n",
      "  smt_dict[variable+'_min'] = np.nanmin(stack_ac_split_arr, axis = 1)\n"
     ]
    }
   ],
   "source": [
    "data_inside = []\n",
    "data_outside = []\n",
    "\n",
    "for path, currentDirectory, files in os.walk(\"data/\"):\n",
    "    for file in files:\n",
    "        if file.startswith(\"data_inside_\") and not file.endswith('_one_hour_example.pkl'):\n",
    "            time_s = time.time()\n",
    "            \n",
    "            unpickled = pd.read_pickle(path+file)\n",
    "            # find starting time of recording\n",
    "            start_time = find_start_date(file)\n",
    "            # find indices/step size for smoothing\n",
    "            step = cutting_points(unpickled)\n",
    "            # optional: add normalized vectors for sensors to data\n",
    "            #unpickled = add_normalized_sensors(unpickled)\n",
    "            # split and calculate values for 20 Hz\n",
    "            smt_dict = smoothing(unpickled, df_no_audio, step)\n",
    "            # export to csv\n",
    "            save_to_csv(smt_dict, start_time, inside=True)\n",
    "            \n",
    "            print('executed \"', path+file, '\" in %s seconds' % (time.time() - time_s))\n",
    "            \n",
    "        elif file.startswith(\"data_outside_\") and not file.endswith('_one_hour_example.pkl'):\n",
    "            time_s = time.time()\n",
    "            \n",
    "            unpickled = pd.read_pickle(path+file)\n",
    "            # find starting time of recording\n",
    "            start_time = find_start_date(file)\n",
    "            # find indices/step size for smoothing\n",
    "            i_split_arr, step = cutting_points(unpickled)\n",
    "            # optional: add normalized vectors for sensors to data\n",
    "            unpickled = add_normalized_sensors(unpickled)\n",
    "            # split and calculate values for 20 Hz\n",
    "            smt_dict = smoothing(unpickled, i_split_arr, step)\n",
    "            # export to csv\n",
    "            save_to_csv(smt_dict, start_time, inside=False)\n",
    "            \n",
    "            print('executed \"', path+file, '\" in %s seconds' % (time.time() - time_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T10:20:49.158141Z",
     "start_time": "2021-11-29T10:20:47.874863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerometer_x_max :  1725554\n",
      "accelerometer_x_95q :  1725554\n",
      "accelerometer_x_min :  1725554\n",
      "accelerometer_x_05q :  1725554\n",
      "accelerometer_y_max :  1725554\n",
      "accelerometer_y_95q :  1725554\n",
      "accelerometer_y_min :  1725554\n",
      "accelerometer_y_05q :  1725554\n",
      "accelerometer_z_max :  1725554\n",
      "accelerometer_z_95q :  1725554\n",
      "accelerometer_z_min :  1725554\n",
      "accelerometer_z_05q :  1725554\n",
      "magnetometer_x_mean :  1725554\n",
      "magnetometer_x_med :  1725554\n",
      "magnetometer_y_mean :  1725554\n",
      "magnetometer_y_med :  1725554\n",
      "magnetometer_z_mean :  1725554\n",
      "magnetometer_z_med :  1725554\n",
      "audio_max :  1725554\n",
      "audio_95q :  1725554\n",
      "timer :  1725553\n",
      "accelerometer_max :  1725554\n",
      "accelerometer_95q :  1725554\n",
      "accelerometer_min :  1725554\n",
      "accelerometer_05q :  1725554\n",
      "magnetometer_mean :  1725554\n",
      "magnetometer_med :  1725554\n",
      "86277650\n"
     ]
    }
   ],
   "source": [
    "#Determine number of sensor records\n",
    "for key in smt_dict.keys():\n",
    "    print (key, ': ', smt_dict[key].size)\n",
    "\n",
    "print(unpickled['timer'].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
